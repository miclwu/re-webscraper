from bs4 import BeautifulSoup
import requests
import hashlib
import time
import sqlite3
from requests.exceptions import HTTPError, Timeout
from utilities import csv_to_dict, dict_to_csv, dbtable_to_dict, dict_update_dbtable
from utilities import db_validate_fieldnames, db_insert, db_update, db_delete

# TODO:
# - cleanup and refactor code
# - implement precheck before executing MOD commands to ensure page changes made b/w command
#       submission and execution are caught
# - improve multi-URL functionality
#   - map URLs to checksums
#   - e.g. two URLs, 1st URL checksum is NULL, 2nd is set. 1st checksum gets set. Program incorrectly detects
#       change in checksum (checksum2 -> checksum1checksum2)
# - revamp input/output system (function and tolerance)
#   - output changed funds to a separate csv file as well, accept input for manual fund status change X
#   - input:
#       - add funds X
#       - delete funds X
#       - modify funds X
#       - get funds table
# - report fund amount if possible
#   - 2 new fields: fund amount and path/id to html element containing fund amount (or maybe find it thru api?)
# - deployment
#   - AWS server w/ simple SQL database?
#   - privileged users need to be able to request/modify data
# - use xlsx instead of csv?
# - checksum specific element(s) instead of checking <body> text
# - 2-bit predictor?
# - proxies
# - NOTE: scraper does not detect page changes if only text generated by js was changed.

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Accept": "text/html,application/xhtml+xml",
    "Accept-Language": "en-US,en;q=0.9"
}

FIELDNAMES = ["name", "url", "status", "urls_to_check"]

OPEN = "Open"
CLOSED = "Closed"
CHECK = "Check Required"

STATUSES = [OPEN, CLOSED, CHECK]

DELIM = ";;"

DATABASE = "webscraper.db"
FUNDS_TABLE = "funds"
INFILE = "input.csv"
OUTFILE = "checkfunds.csv"
AUDITLOG = "auditlog.txt"

def precheck():
    try:
        inputs = csv_to_dict(INFILE)
    except FileNotFoundError as e:
        print(f'FileNotFoundError: {e}. No inputs.')
        return None
    return inputs

def get_soup(url, retries= 3, backoff= 2):
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=(3.1, 15.1))
            if 'text/html' not in response.headers['content-type']:
                raise TypeError
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            return soup
        
        except Timeout as e:
            wait_time = backoff ** attempt
            print(f'TimeoutError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except TypeError as e:
            wait_time = backoff ** attempt
            print(f'TypeError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except HTTPError as e:
            if e.response.status_code == 412:
                print(f'Error 412. Skipping "{url}"...')
                return None
            wait_time = backoff ** attempt
            print(f'HTTP error: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)
            
        except Exception as e:
            print(f'Failed to reach URL: {url}. Error: {e}. Skipping...')
            return None
    return None

def main():
    inputs = precheck()

    if inputs:

        conn = sqlite3.connect(DATABASE)
        auditlog = open(AUDITLOG, "w")

        for item in inputs:
            if list(item.keys()) != ["command", "name", "url", "status"]:
                auditlog.write('INPUT FILE ERROR: Invalid set of column headers\r\n')
                break

            cmd = item.pop("command").upper()

            if item["status"] not in STATUSES and cmd != "DEL":
                auditlog.write(f'INPUT ERROR: Invalid status: "{item["status"]}" for command: {item["command"].upper()} {item["name"]}\r\n')
                continue
            try:
                if cmd == "ADD":
                    db_insert(conn, FUNDS_TABLE, item)
                    auditlog.write(f'ADD: {item["name"]}\r\n')
                elif cmd == "MOD":
                    # TODO: notify user if fund DNE
                    item['checksum'] = None
                    item['urls_to_check'] = None
                    item['access_failures'] = 0
                    db_update(conn, FUNDS_TABLE, item)
                    auditlog.write(f'MOD: {item["name"]}, {item["url"]}, {item["status"]}\r\n')
                elif cmd == "DEL":
                    db_delete(conn, FUNDS_TABLE, "name", item["name"])
                    auditlog.write(f'DEL: {item["name"]}\r\n')
                else:
                    auditlog.write(f'INPUT ERROR: Invalid command: {cmd.upper()} {item["name"]}\r\n')
                    continue
            except sqlite3.IntegrityError as e:
                auditlog.write(f'INTEGRITY ERROR: {cmd} {item["name"]}: {e}\r\n')
            except Exception as e:
                print(f'Encountered exception: {e}')

        conn.close()
        auditlog.close()

    funds = dbtable_to_dict(DATABASE, FUNDS_TABLE)
    funds_to_check = []
    funds_to_update = []

    for item in funds:
        assert(item["name"])
        assert(item["url"])
        assert(item["status"] == OPEN or item["status"] == CLOSED or item["status"] == CHECK)
        urls = item['url'].split(DELIM)
        urls_to_check = set(item['urls_to_check'].split(DELIM)) if item['urls_to_check'] else set()
        old_checksums = item["checksum"].split(DELIM) if item["checksum"] else ['' for u in urls]
        checksums = []

        need_update = False
        failed_connect = False
        for i in range(len(urls)):

            soup = get_soup(urls[i])

            if not soup:
                
                need_update = True
                failed_connect = True
                item["access_failures"] += 1
        
                if item["access_failures"] >= 3:
                    urls_to_check.add(urls[i])
                
                checksums.append(old_checksums[i])
                print(f"{item['name']}, {item['status'].upper()} FUND: URL {i+1}: Failed to connect.")
            else:
                checksums.append(hashlib.sha256("".join(soup.body.text.split()).encode('utf-8')).hexdigest())
            
                if not old_checksums[i]:
                    need_update = True
                    print(f'{item["name"]}, {item["status"].upper()} FUND: URL {i+1}: Adding new checksum.')
                elif checksums[i] != old_checksums[i]:
                    need_update = True
                    urls_to_check.add(urls[i])
                    print(f'{item["name"]}, {item["status"].upper()} FUND: URL {i+1}: Updating checksum. Check required.')
                else:
                    print(f'{item["name"]}, {item["status"].upper()} FUND: URL {i+1}: Checksums match.')
        
        if not failed_connect:
            item['access_failures'] = 0            
        
        if need_update:
            item['checksum'] = DELIM.join(checksums)
            funds_to_update.append(item)
        
        if urls_to_check:
            item['status'] = CHECK
            item['urls_to_check'] = DELIM.join(urls_to_check)
        
        if item['status'] == CHECK:
            funds_to_check.append(item)
    
    dict_update_dbtable(funds_to_update, DATABASE, FUNDS_TABLE, ['status', 'checksum', 'urls_to_check', 'access_failures'])
    dict_to_csv(funds_to_check, OUTFILE, FIELDNAMES)

if __name__ == "__main__":
    main()
