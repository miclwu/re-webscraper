from bs4 import BeautifulSoup
import requests
import sqlite3
import pandas as pd
import hashlib
import os
import shutil
import time
from requests.exceptions import HTTPError, Timeout
import utilities as util
from typing import Any
from io import TextIOWrapper
from constants import *

# TODO:
# - input:
#   - restore table?
# - backup system
# - report fund amount if possible
#   - 2 new fields: fund amount and path/id to html element containing fund amount (or maybe find it thru api?)
# - deployment
# - regex ADDU user emails, fund URLs
# - update README with audit log explanation
# - NOTE: scraper does not detect page changes if only text generated by js was changed.

def queue_inputs(
    log: TextIOWrapper
) -> list[dict[str, Any]]:
    """Convert input files (xlsx) located in `INFILE_DIR` to records (list of dicts)

    Each row of an input file contains the columns: 'command', 'name', 'url', 'status' at minimum.
    Errors are printed to the audit log, `log`.

    Args:
        log: The open audit log file to write to
    Returns:
        A list of dicts, with each dict representing an input command
    """
    if not os.path.isdir(INFILE_DIR):
        log.write('INPUT FILE ERROR: Unable to locate input directory\n\n')
        print(f"Input directory not found. Unable to locate inputs.")

    inputs = []
    i = 1
    while True:
        infile = f"{INFILE_DIR}/{INFILE_TEMPLATE.replace('X', str(i))}"
        if not os.path.isfile(infile):
            log.write(f"INFO: Input files: {i - 1}\n\n")
            print(f"Input files: {i - 1}")
            break
        try:
            inputs.extend(util.xlsx_to_records(infile, usecols=INPUT_COLS))
        except ValueError as e:
            log.write('INPUT FILE ERROR: Incomplete set of column headers, requires: (command, name, url, status)\n\n')
            print(f"webscraper.py: queue_inputs(): Invalid column headers")
        except Exception as e:
            log.write('INPUT FILE ERROR: Unable to parse file\n\n')
            print(f"webscraper.py: queue_inputs(): Exception: {e}")
            break
        i += 1
    return inputs

def exec_cmd(
    conn: sqlite3.Connection,
    log: TextIOWrapper,
    item: dict[str, Any],
    table_reqs: set[str]
) -> None:
    """Execute the command, represented by the dict `item`, on the database.

    Validate command before execution. Print executed commands (or error messages) to `log`.
    If command is a table request ('REQ'), append requested table name to `table_reqs`, if
    valid.

    Args:
        conn: An open connection to an sqlite3 database
        log: The open audit log file to write to
        item: A dict representing an operation to be performed on the database
        table_reqs: List of requested table names
    """
    cmd = item.pop('command').upper()

    if cmd not in ('ADD', 'MOD', 'DEL', 'REQ', 'REQB', 'ADDU', 'DELU', 'BACKUP', 'RESTORE'):
        log.write(f"INPUT ERROR: Invalid command: \"{cmd} {item['name']}\"\n\n")
        return
    if not item['name'] and cmd not in ('BACKUP', 'RESTORE'):
        log.write(f"INPUT ERROR: {cmd}: Empty name\n\n")
        return
    if not item['url'] and cmd == 'ADD':
        log.write(f"INPUT ERROR: {cmd} {item['name']}: Empty URL\n\n")
        return
    if (
        (cmd in ('ADD', 'MOD') and item['status'] not in STATUSES) or 
        (cmd == 'ADDU' and item['status'] not in (True, False, 1, 0))
    ):
        log.write(f"INPUT ERROR: {cmd} {item['name']}: Invalid status: \"{item['status']}\"\n\n")
        return
    
    try:
        if cmd == 'ADD':
            util.db_insert(conn, FUNDS_TABLE, item)
            log.write(f"ADD: {item['name']}\n\n")

        elif cmd == 'MOD':
            # Precheck to catch page changes since last check
            item_old = util.db_get_row(conn, FUNDS_TABLE, DB_FUNDS_COLS, key='name', val=item['name'])
            if not item_old:
                log.write(f"INPUT ERROR: {item['name']} does not exist for command MOD\n\n")
                return
            funds_to_check = []
            check_fund(None, item_old, funds_to_check, [])
            if funds_to_check:
                log.write(f"WARNING: Potential change to fund \"{item['name']}\" before MOD command\n")

            # Execute MOD
            # Remove url field if not set
            if not item['url']:
                item.pop('url')

            # Reset checksum, urls_to_check, and access_failures fields
            item['checksum'] = None
            item['urls_to_check'] = None
            item['access_failures'] = 0
            util.db_update(conn, FUNDS_TABLE, item, key='name')
            log.write(f"MOD: {item['name']}, {item['url']}, {item['status']}\n\n")

        elif cmd == 'DEL':
            util.db_delete(conn, FUNDS_TABLE, key='name', val=item['name'])
            log.write(f"DEL: {item['name']}\n\n")
        
        elif cmd == 'REQ':
            table_name = item['name'].lower()
            util.db_validate_table(conn, table_name)
            table_reqs.add(table_name)
            log.write(f"REQ: Table \"{table_name}\"\n\n")
        
        elif cmd == 'REQB':
            conn_backup = sqlite3.connect(DATABASE_BACKUP)

            table_name = item['name'].lower()
            util.db_validate_table(conn_backup, table_name)
            table_reqs.add(f"backup{DELIM}{table_name}")
            log.write(f"REQB: Table \"{table_name}\"\n\n")

            conn_backup.close()
        
        elif cmd == 'ADDU':
            util.db_insert(conn, USERS_TABLE, {'email': item['name'], 'admin': item['status']})
            log.write(f"ADDU: {item['name']}, admin: {item['status']}\n\n")

        elif cmd == 'DELU':
            util.db_delete(conn, USERS_TABLE, key='email', val=item['name'])
            log.write(f"DELU: {item['name']}\n\n")

        elif cmd == 'BACKUP':
            conn_backup = sqlite3.connect(DATABASE_BACKUP)
            conn.backup(conn_backup)
            log.write(f"BACKUP: Saved backup of database\n\n")
            conn_backup.close()
            
        else:   # cmd == 'RESTORE'
            shutil.copyfile(src=DATABASE_BACKUP, dst=DATABASE)
            table_reqs.add(FUNDS_TABLE)
            table_reqs.add(USERS_TABLE)
            log.write(f"RESTORE: Restored database from backup. Added database tables to output\n\n")

    except sqlite3.Error as e:
        conn.rollback()
        log.write(f"DATABASE ERROR: {cmd} {item['name']}: {e}\n\n")
    except util.InvalidInputError as e:
        log.write(f"INPUT ERROR: {cmd} {item['name']}: {e}\n\n")
    except Exception as e:
        print(f"Encountered exception: {e}")

def get_soup(
    url: str,
    retries= 3,
    backoff= 2
) -> BeautifulSoup | None:
    """Scrape html from `url` and return BeautifulSoup object, if possible.

    Args:
        url: The url to scrape
        retries: The number of retry attempts, if html request fails
        backoff: The backoff factor, used to calculate wait time between retries
    Returns:
        A BeautifulSoup object of the html from `url`, or None on failure
    """
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=HTTP_GET_HEADERS, timeout=(3.1, 15.1))
            if 'text/html' not in response.headers['content-type']:
                raise TypeError
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            return soup
        
        except Timeout as e:
            wait_time = backoff ** attempt
            print(f'TimeoutError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except TypeError as e:
            wait_time = backoff ** attempt
            print(f'TypeError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except HTTPError as e:
            if e.response.status_code == 412:
                print(f"Error 412. Skipping \"{url}\"...")
                return None
            wait_time = backoff ** attempt
            print(f"HTTP error: {e}. Retrying in {wait_time} seconds...")
            time.sleep(wait_time)
            
        except Exception as e:
            print(f"Failed to reach URL: {url}. Error: {e}. Skipping...")
            return None
    return None

def check_fund(
    log: TextIOWrapper | None,
    fund: dict[str, Any],
    funds_to_check: list[dict[str, Any]],
    funds_to_update: list[dict[str, Any]]
) -> None:
    """Check a fund for page changes by comparing page checksum data for each url.

    If page change detected, append `fund` to `funds_to_check`. Append `fund` to
    `funds_to_update` if any column in `fund` does not match the same column for the
    version of `fund` in the database. Does not update the database.

    Args:
        log: The open audit log file to write to, or None to suppress logging
        fund: A dict representing a fund's info / a row in the database
        funds_to_check: A list of funds that need to be checked
        funds_to_update: A list of funds that need to be updated
    """
    urls = fund['url'].split(DELIM)
    urls_to_check = set(fund['urls_to_check'].split(DELIM)) if fund['urls_to_check'] else set()
    old_checksums = fund['checksum'].split(DELIM) if fund['checksum'] else ['' for u in urls]
    checksums = []

    need_update = False
    failed_connect = False
    for i in range(len(urls)):

        soup = get_soup(urls[i])

        if not soup:
            # Unable to scrape url
            need_update = True
            failed_connect = True
            fund['access_failures'] += 1
    
            if fund['access_failures'] >= 3:
                urls_to_check.add(urls[i])
            
            checksums.append(old_checksums[i])

            if log:
                log.write(f"SCRAPE FAIL: {fund['name']}, {urls[i]}\n\n")

            print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Failed to connect.")
        else:
            # Successful url scrape
            # Remove page whitespace, checksum remaining text and add to list
            checksums.append(hashlib.sha256(''.join(soup.body.text.split()).encode('utf-8')).hexdigest())

            if not old_checksums[i]:
                need_update = True
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Adding new checksum.")
            elif checksums[i] != old_checksums[i]:
                need_update = True
                urls_to_check.add(urls[i])

                if log:
                    log.write(f"CHECK: {fund['name']}, {urls[i]}\n\n")
                
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Updating checksum. Check required.")
            else:
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Checksums match.")
    
    if not failed_connect:
        fund['access_failures'] = 0
    
    if need_update:
        fund['checksum'] = DELIM.join(checksums)
        funds_to_update.append(fund)
    
    if urls_to_check:
        fund['status'] = CHECK
        fund['urls_to_check'] = DELIM.join(urls_to_check)
    
    if fund['status'] == CHECK:
        funds_to_check.append(fund)

def main(
    conn: sqlite3.Connection,
    log: TextIOWrapper
) -> None:
    """Execute inputs, scrape urls, update database, and generate output files.

    Compare html from each scraped url with saved data in database. Update and mark
    database entries when changes in html occur.

    Args:
        conn: An open connection to an sqlite3 database
        log: The open audit log file to write to
    """
    inputs = queue_inputs(log)

    table_reqs = set()
    for item in inputs:
        exec_cmd(conn, log, item, table_reqs)

    funds = util.dbtable_to_records(conn, FUNDS_TABLE)
    funds_to_check = []
    funds_to_update = []

    for item in funds:
        check_fund(log, item, funds_to_check, funds_to_update)
    
    if funds_to_update:
        try:
            util.records_update_dbtable(conn, FUNDS_TABLE, ['status', 'checksum', 'urls_to_check', 'access_failures'], funds_to_update)
        except sqlite3.Error as e:
            conn.rollback()
            log.write(f"DATABASE ERROR: Fatal error when updating database: {e}\n\n")
        except Exception as e:
            log.write(f"ERROR: Occurred when updating database: {e}\n\n")

    if funds_to_check:
        util.records_to_xlsx(funds_to_check, OUTFILE_USER_PATH, OUTPUT_COLS, sheet_name='Funds to Check')

    log.write(f"INFO: Updated funds: {len(funds_to_update)}/{len(funds)}\n\n")
    log.write(f"INFO: Funds to check: {len(funds_to_check)}/{len(funds)}\n\n")

    if table_reqs:
        conn_backup = sqlite3.connect(DATABASE_BACKUP)
        with pd.ExcelWriter(OUTFILE_ADMIN_PATH, engine='openpyxl') as writer:
            if funds_to_check:
                util.records_to_xlsx(funds_to_check, writer, OUTPUT_COLS, sheet_name='Funds to Check')
            for req in table_reqs:
                conn_tmp = conn
                sheet_name = f"Table {req}"
                table_name = str(req)
                
                # check if requested table is from the backup db
                if table_name.find(f"backup{DELIM}") == 0:
                    conn_tmp = conn_backup
                    table_name = table_name.replace(f"backup{DELIM}", '')
                    sheet_name = f"Backup Table {table_name}"

                if req == FUNDS_TABLE and conn_tmp == conn:
                    util.records_to_xlsx(funds, writer, sheet_name=sheet_name)
                else:
                    util.records_to_xlsx(util.dbtable_to_records(conn_tmp, table_name), writer, sheet_name=sheet_name)
        conn_backup.close()

if __name__ == '__main__':
    conn = sqlite3.connect(DATABASE)
    auditlog = open(AUDITLOG_PATH, 'w')
    main(conn, auditlog)
    auditlog.close()
    conn.close()
