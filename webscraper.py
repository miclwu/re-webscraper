from bs4 import BeautifulSoup
import requests
import hashlib
import time
from enum import Enum
from requests.exceptions import HTTPError, Timeout
from utilities import csv_to_dict, dict_to_csv, dbtable_to_dict, dict_update_dbtable

# TODO:
# - cleanup and refactor code
# - revamp input/output system (function and tolerance)
#   - output changed funds to a separate csv file as well, accept input for manual fund status change
#   - OR sort main csv by last changed so manual editing is easier
#   - input:
#       - add funds
#       - delete funds
#       - modify funds
# - report fund amount if possible
#   - 2 new fields: fund amount and path/id to html element containing fund amount (or maybe find it thru api?)
# - deployment
#   - AWS server w/ simple SQL database?
#   - privileged users need to be able to request/modify data
# - use xlsx instead of csv?
# - checksum specific element(s) instead of checking <body> text
# - 2-bit predictor?
# - proxies
# - NOTE: scraper does not detect page changes if only text generated by js was changed.

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Accept": "text/html,application/xhtml+xml",
    "Accept-Language": "en-US,en;q=0.9"
}

FIELDNAMES = ["name", "url", "status", "checksum"]

class Status(Enum):
    OPEN = "Open"
    CLOSED = "Closed"
    CHECK = "Check Required"

DELIM = ";;"

DATABASE = "webscraper.db"
FUNDS_TABLE = "funds"
INFILE = "input.csv"
OUTFILE = "checkfunds.csv"
AUDITLOG = "auditlog.txt"

def precheck():
    try:
        inputs = csv_to_dict(INFILE)
    except FileNotFoundError as e:
        print(f'FileNotFoundError: {e}. No inputs.')
        return None
    return inputs

def get_soup(url, retries= 3, backoff= 2):
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=(3.1, 15.1))
            if 'text/html' not in response.headers['content-type']:
                raise TypeError
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            return soup
        
        except Timeout as e:
            wait_time = backoff ** attempt
            print(f'TimeoutError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except TypeError as e:
            wait_time = backoff ** attempt
            print(f'TypeError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except HTTPError as e:
            if e.response.status_code == 412:
                print(f'Error 412. Skipping "{url}"...')
                return None
            wait_time = backoff ** attempt
            print(f'HTTP error: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)
            
        except Exception as e:
            print(f'Failed to reach URL: {url}. Error: {e}. Skipping...')
            return None
    return None

def main():
    inputs = precheck()

    auditlog = open(AUDITLOG, "w")

    for item in inputs:
        if item.keys() != ["command", "name", "url", "status"]:
            auditlog.write("INPUT FILE ERROR: Invalid set of column headers.")
            break
        if item["command"].upper() not in ["ADD", "MOD", "DEL"]:
            auditlog.write(f"INPUT ERROR: Invalid command: {item["command"].upper()} {item["name"]}")
            continue
        if item["status"] not in ["Open", "Closed", "Check Required"]:
            auditlog.write(f"INPUT ERROR: Invalid status: {item["command"].upper()} for command: {item["command"].upper()} {item["name"]}")
            continue
        

    auditlog.close()

    funds = dbtable_to_dict(DATABASE, FUNDS_TABLE)
    funds_to_check = []
    funds_to_update = []

    for item in funds:
        assert(item["name"])
        assert(item["url"])
        assert(item["status"] == Status.OPEN.value or item["status"] == Status.CLOSED.value or item["status"] == Status.CHECK.value)

        old_checksum = item["checksum"]
        checksum = ""

        for url in item["url"].split(DELIM):
        
            soup = get_soup(url)

            if not soup:
                item["access_failures"] += 1
                funds_to_update.append(item)
                if item["access_failures"] >= 3:
                    funds_to_check.append(item)
                continue

            item["access_failures"] = 0
            
            checksum += hashlib.sha256("".join(soup.body.text.split()).encode('utf-8')).hexdigest()
        
        if not old_checksum:
            item["checksum"] = checksum
            print(f'{item["name"]}, {item["status"].upper()} FUND: Adding new checksum.')
            funds_to_update.append(item)
            continue
        
        if checksum != old_checksum:
            item["status"] = Status.CHECK.value
            item["checksum"] = checksum
            funds_to_update.append(item)
            print(f'{item["name"]}, {item["status"].upper()} FUND: Page change detected. Updating checksum. Check required.')
        else:
            print(f'{item["name"]}, {item["status"].upper()} FUND: Checksums match.')
        
        if item["status"] == Status.CHECK.value:
            funds_to_check.append(item)
    
    dict_update_dbtable(funds_to_update, DATABASE, FUNDS_TABLE, ["status", "checksum", "access_failures"])
    dict_to_csv(funds_to_check, OUTFILE, FIELDNAMES)

if __name__ == "__main__":
    main()
