from bs4 import BeautifulSoup
import requests
import hashlib
import os
import time
import sqlite3
from requests.exceptions import HTTPError, Timeout
from utilities import csv_to_dict, dict_to_csv, dbtable_to_dict, dict_update_dbtable
from utilities import db_insert, db_update, db_delete, db_get_row

# TODO:
# - revamp input/output system (function and tolerance)
#   - output changed funds to a separate csv file as well, accept input for manual fund status change X
#   - input:
#       - add funds X
#       - delete funds X
#       - modify funds X
#       - get funds table
# - report fund amount if possible
#   - 2 new fields: fund amount and path/id to html element containing fund amount (or maybe find it thru api?)
# - deployment
#   - AWS server w/ simple SQL database?
#   - privileged users need to be able to request/modify data
# - use xlsx instead of csv
# - proxies
# - NOTE: scraper does not detect page changes if only text generated by js was changed.

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept': 'text/html,application/xhtml+xml',
    'Accept-Language': 'en-US,en;q=0.9'
}

INPUT_COLS = ('command', 'name', 'url', 'status')
OUTPUT_COLS = ('name', 'url', 'status', 'urls_to_check')

OPEN = 'Open'
CLOSED = 'Closed'
CHECK = 'Check Required'

STATUSES = (OPEN, CLOSED, CHECK)

DELIM = ';;'

DATABASE = 'webscraper.db'
FUNDS_TABLE = 'funds'
INFILE_DIR = 'inputs'
INFILE_TEMPLATE  = 'input_X.xlsx'
OUTFILE = 'checkfunds.xlsx'
AUDITLOG = 'auditlog.txt'

# Looks for input files (csv) in the directory INFILE_DIR
# Converts each line of each file into a dict representing a command
# Returns the list of commands
def queue_inputs():
    if not os.path.isdir(INFILE_DIR):
        print(f"Input directory not found. Unable to locate inputs.")

    inputs = []
    i = 1
    while True:
        infile = f"{INFILE_DIR}/{INFILE_TEMPLATE.replace('X', str(i))}"
        if not os.path.isfile(infile):
            print(f"Input files: {i - 1}")
            break
        try:
            inputs.extend(xlsx_to_records(infile, usecols=INPUT_COLS))
        except Exception as e:
            print(f"webscraper.py: queue_inputs(): Exception: {e}")
            break
        i += 1
    return inputs

# Executes the command, 'item', performing corresponding operation on db
# Prints executed commands (or error messages) to 'log'
def exec_cmd(conn, log, item):
    if set(item.keys()) != {'command', 'name', 'url', 'status'}:
        log.write('INPUT FILE ERROR: Invalid set of column headers\r\n')
        return

    cmd = item.pop('command').upper()

    if not item['name']:
        log.write(f"INPUT ERROR: Empty name for command {cmd}\r\n")
        return
    if not item['url']:
        log.write(f"INPUT ERROR: Empty URL for command {cmd} {item['name']}\r\n")
        return
    if item['status'] not in STATUSES and cmd != 'DEL':
        log.write(f"INPUT ERROR: Invalid status: \"{item['status']}\" for command: {cmd} {item['name']}\r\n")
        return
    
    try:
        if cmd == 'ADD':
            db_insert(conn, FUNDS_TABLE, item)
            log.write(f"ADD: {item['name']}\r\n")

        elif cmd == 'MOD':
            # Precheck to catch page changes since last check
            item_old = db_get_row(conn, FUNDS_TABLE, ('name', 'url', 'status', 'checksum', 'urls_to_check', 'access_failures'), item['name'])
            if not item_old:
                log.write(f"COMMAND ERROR: {item['name']} does not exist for command MOD\r\n")
                return
            funds_to_check, temp = check_fund(item_old, [], [])
            if funds_to_check:
                log.write(f"WARNING: Potential change to fund {item['name']} before MOD command\n")

            # Execute MOD
            item['checksum'] = None
            item['urls_to_check'] = None
            item['access_failures'] = 0
            db_update(conn, FUNDS_TABLE, item)
            log.write(f"MOD: {item['name']}, {item['url']}, {item['status']}\r\n")

        elif cmd == 'DEL':
            db_delete(conn, FUNDS_TABLE, 'name', item['name'])
            log.write(f"DEL: {item['name']}\r\n")

        else:
            log.write(f"INPUT ERROR: Invalid command: {cmd.upper()} {item['name']}\r\n")

    except sqlite3.IntegrityError as e:
        log.write(f"INTEGRITY ERROR: {cmd} {item['name']}: {e}\r\n")
    except Exception as e:
        print(f"Encountered exception: {e}")

# Connects to 'url' and returns bs4 object, if possible
# Retries up to 'retries' times with a backoff factor of 'backoff'
# Returns None on failure
def get_soup(url, retries= 3, backoff= 2):
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=(3.1, 15.1))
            if 'text/html' not in response.headers['content-type']:
                raise TypeError
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            return soup
        
        except Timeout as e:
            wait_time = backoff ** attempt
            print(f'TimeoutError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except TypeError as e:
            wait_time = backoff ** attempt
            print(f'TypeError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except HTTPError as e:
            if e.response.status_code == 412:
                print(f"Error 412. Skipping \"{url}\"...")
                return None
            wait_time = backoff ** attempt
            print(f"HTTP error: {e}. Retrying in {wait_time} seconds...")
            time.sleep(wait_time)
            
        except Exception as e:
            print(f"Failed to reach URL: {url}. Error: {e}. Skipping...")
            return None
    return None

# Checks a fund for page changes by comparing page checksum data for each url
# If page change was detected, appends 'fund' to 'funds_to_check'
# If any field for 'fund' needs to be updated in the db, appends it to 'funds_to_update'
# Returns the updated lists of 'funds_to_check' and 'funds_to_update'
def check_fund(fund, funds_to_check, funds_to_update):
    urls = fund['url'].split(DELIM)
    urls_to_check = set(fund['urls_to_check'].split(DELIM)) if fund['urls_to_check'] else set()
    old_checksums = fund['checksum'].split(DELIM) if fund['checksum'] else ['' for u in urls]
    checksums = []

    need_update = False
    failed_connect = False
    for i in range(len(urls)):

        soup = get_soup(urls[i])

        if not soup:
            # Unable to scrape url
            need_update = True
            failed_connect = True
            fund['access_failures'] += 1
    
            if fund['access_failures'] >= 3:
                urls_to_check.add(urls[i])
            
            checksums.append(old_checksums[i])
            print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Failed to connect.")
        else:
            # Successful url scrape
            # Checksum remove page whitespace, checksum remaining text and add to list
            checksums.append(hashlib.sha256(''.join(soup.body.text.split()).encode('utf-8')).hexdigest())

            if not old_checksums[i]:
                need_update = True
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Adding new checksum.")
            elif checksums[i] != old_checksums[i]:
                need_update = True
                urls_to_check.add(urls[i])
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Updating checksum. Check required.")
            else:
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Checksums match.")
    
    if not failed_connect:
        fund['access_failures'] = 0
    
    if need_update:
        fund['checksum'] = DELIM.join(checksums)
        funds_to_update.append(fund)
    
    if urls_to_check:
        fund['status'] = CHECK
        fund['urls_to_check'] = DELIM.join(urls_to_check)
    
    if fund['status'] == CHECK:
        funds_to_check.append(fund)

    return funds_to_check, funds_to_update

# Main loop:
# Queue input commands
# Execute input commands
# Check each fund in database
# Update database
# Write funds that need to be checked to output file
def main():
    inputs = queue_inputs()

    conn = sqlite3.connect(DATABASE)

    if inputs:
        auditlog = open(AUDITLOG, 'w')

        for item in inputs:
            exec_cmd(conn, auditlog, item)

        auditlog.close()

    funds = dbtable_to_dict(conn, FUNDS_TABLE)
    funds_to_check = []
    funds_to_update = []

    for item in funds:
        funds_to_check, funds_to_update = check_fund(item, funds_to_check, funds_to_update)
    
    if funds_to_update:
        dict_update_dbtable(conn, FUNDS_TABLE, ['status', 'checksum', 'urls_to_check', 'access_failures'], funds_to_update)
    if funds_to_check:
        records_to_xlsx(funds_to_check, OUTFILE, OUTPUT_COLS)

    conn.close()

if __name__ == '__main__':
    main()
