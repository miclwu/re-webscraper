from bs4 import BeautifulSoup
import requests
import sqlite3
import pandas as pd
import hashlib
import os
import time
from requests.exceptions import HTTPError, Timeout
from utilities import xlsx_to_records, records_to_xlsx, dbtable_to_records, records_update_dbtable
from utilities import db_validate_table, db_insert, db_update, db_delete, db_get_row
from utilities import InvalidInputError
from typing import Any
from constants import *

# TODO:
# - revamp input/output system (function and tolerance)
#   - output changed funds to a separate csv file as well, accept input for manual fund status change X
#   - input:
#       - add funds X
#       - delete funds X
#       - modify funds X
#       - get funds table
#       - restore table?
# - backup system
# - email input/output script
#   - validate users before processing input
# - report fund amount if possible
#   - 2 new fields: fund amount and path/id to html element containing fund amount (or maybe find it thru api?)
# - deployment
#   - AWS server w/ simple SQL database?
#   - privileged users need to be able to request/modify data
# - use xlsx instead of csv
# - proxies
# - NOTE: scraper does not detect page changes if only text generated by js was changed.

def queue_inputs(
    log: str
) -> list[dict[str, Any]]:
    """Convert input files (xlsx) located in `INFILE_DIR` to records (list of dicts)

    Each row of an input file contains the columns: 'command', 'name', 'url', 'status' at minimum.
    Errors are printed to the audit log, `log`.

    Args:
        log: The name of the opened audit log to be written to
    Returns:
        A list of dicts, with each dict representing an input command
    """
    if not os.path.isdir(INFILE_DIR):
        print(f"Input directory not found. Unable to locate inputs.")

    inputs = []
    i = 1
    while True:
        infile = f"{INFILE_DIR}/{INFILE_TEMPLATE.replace('X', str(i))}"
        if not os.path.isfile(infile):
            print(f"Input files: {i - 1}")
            break
        try:
            inputs.extend(xlsx_to_records(infile, usecols=INPUT_COLS))
        except ValueError as e:
            log.write('INPUT FILE ERROR: Incomplete set of column headers, requires: (command, name, url, status)\r\n')
            print(f"webscraper.py: queue_inputs(): Invalid column headers")
        except Exception as e:
            log.write('INPUT FILE ERROR: Unable to parse file\r\n')
            print(f"webscraper.py: queue_inputs(): Exception: {e}")
            break
        i += 1
    return inputs

def exec_cmd(
    conn: sqlite3.Connection,
    log: str,
    item: dict[str, Any]
) -> None:
    """Execute the command, represented by the dict `item`, on the database.

    Validate command before execution. Print executed commands (or error messages) to `log`.

    Args:
        conn: An open connection to an sqlite3 database
        log: The name of the opened audit log to be written to
        item: A dict representing an operation to be performed on the database
    """
    cmd = item.pop('command').upper()

    if cmd not in ('ADD', 'MOD', 'DEL'):
        log.write(f"INPUT ERROR: Invalid command: {cmd.upper()} {item['name']}\r\n")
        return
    if not item['name']:
        log.write(f"INPUT ERROR: Empty name for command {cmd}\r\n")
        return
    if not item['url']:
        log.write(f"INPUT ERROR: Empty URL for command {cmd} {item['name']}\r\n")
        return
    if item['status'] not in STATUSES and cmd != 'DEL':
        log.write(f"INPUT ERROR: Invalid status: \"{item['status']}\" for command: {cmd} {item['name']}\r\n")
        return
    
    try:
        if cmd == 'ADD':
            db_insert(conn, FUNDS_TABLE, item)
            log.write(f"ADD: {item['name']}\r\n")

        elif cmd == 'MOD':
            # Precheck to catch page changes since last check
            item_old = db_get_row(conn, FUNDS_TABLE, ('name', 'url', 'status', 'checksum', 'urls_to_check', 'access_failures'), item['name'])
            if not item_old:
                log.write(f"COMMAND ERROR: {item['name']} does not exist for command MOD\r\n")
                return
            funds_to_check = []
            check_fund(item_old, funds_to_check, [])
            if funds_to_check:
                log.write(f"WARNING: Potential change to fund {item['name']} before MOD command\n")

            # Execute MOD
            item['checksum'] = None
            item['urls_to_check'] = None
            item['access_failures'] = 0
            db_update(conn, FUNDS_TABLE, item)
            log.write(f"MOD: {item['name']}, {item['url']}, {item['status']}\r\n")

        else:   # cmd == 'DEL'
            db_delete(conn, FUNDS_TABLE, 'name', item['name'])
            log.write(f"DEL: {item['name']}\r\n")

    except sqlite3.IntegrityError as e:
        log.write(f"INTEGRITY ERROR: {cmd} {item['name']}: {e}\r\n")
    except Exception as e:
        print(f"Encountered exception: {e}")

def get_soup(
    url: str,
    retries= 3,
    backoff= 2
) -> BeautifulSoup | None:
    """Scrape html from `url` and return BeautifulSoup object, if possible.

    Args:
        url: The url to scrape
        retries: The number of retry attempts, if html request fails
        backoff: The backoff factor, used to calculate wait time between retries
    Returns:
        A BeautifulSoup object of the html from `url`, or None on failure
    """
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=(3.1, 15.1))
            if 'text/html' not in response.headers['content-type']:
                raise TypeError
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            return soup
        
        except Timeout as e:
            wait_time = backoff ** attempt
            print(f'TimeoutError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except TypeError as e:
            wait_time = backoff ** attempt
            print(f'TypeError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except HTTPError as e:
            if e.response.status_code == 412:
                print(f"Error 412. Skipping \"{url}\"...")
                return None
            wait_time = backoff ** attempt
            print(f"HTTP error: {e}. Retrying in {wait_time} seconds...")
            time.sleep(wait_time)
            
        except Exception as e:
            print(f"Failed to reach URL: {url}. Error: {e}. Skipping...")
            return None
    return None

def check_fund(
    fund: dict[str, Any],
    funds_to_check: list[dict[str, Any]],
    funds_to_update: list[dict[str, Any]]
) -> None:
    """Check a fund for page changes by comparing page checksum data for each url.

    If page change detected, append `fund` to `funds_to_check`. Append `fund` to
    `funds_to_update` if any column in `fund` does not match the same column for the
    version of `fund` in the database.

    Args:
        fund: A dict representing a fund's info / a row in the database
        funds_to_check: A list of funds that need to be checked
        funds_to_update: A list of funds that need to be updated
    """
    urls = fund['url'].split(DELIM)
    urls_to_check = set(fund['urls_to_check'].split(DELIM)) if fund['urls_to_check'] else set()
    old_checksums = fund['checksum'].split(DELIM) if fund['checksum'] else ['' for u in urls]
    checksums = []

    need_update = False
    failed_connect = False
    for i in range(len(urls)):

        soup = get_soup(urls[i])

        if not soup:
            # Unable to scrape url
            need_update = True
            failed_connect = True
            fund['access_failures'] += 1
    
            if fund['access_failures'] >= 3:
                urls_to_check.add(urls[i])
            
            checksums.append(old_checksums[i])
            print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Failed to connect.")
        else:
            # Successful url scrape
            # Checksum remove page whitespace, checksum remaining text and add to list
            checksums.append(hashlib.sha256(''.join(soup.body.text.split()).encode('utf-8')).hexdigest())

            if not old_checksums[i]:
                need_update = True
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Adding new checksum.")
            elif checksums[i] != old_checksums[i]:
                need_update = True
                urls_to_check.add(urls[i])
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Updating checksum. Check required.")
            else:
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Checksums match.")
    
    if not failed_connect:
        fund['access_failures'] = 0
    
    if need_update:
        fund['checksum'] = DELIM.join(checksums)
        funds_to_update.append(fund)
    
    if urls_to_check:
        fund['status'] = CHECK
        fund['urls_to_check'] = DELIM.join(urls_to_check)
    
    if fund['status'] == CHECK:
        funds_to_check.append(fund)

# Main loop:
# Queue input commands
# Execute input commands
# Check each fund in database
# Update database
# Write funds that need to be checked to output file
def main():
    auditlog = open(AUDITLOG, 'w')
    inputs = queue_inputs(auditlog)

    conn = sqlite3.connect(DATABASE)

    auditlog = open(AUDITLOG, 'w')

    for item in inputs:
        exec_cmd(conn, auditlog, item)

    auditlog.close()

    funds = dbtable_to_records(conn, FUNDS_TABLE)
    funds_to_check = []
    funds_to_update = []

    for item in funds:
        check_fund(item, funds_to_check, funds_to_update)
    
    if funds_to_update:
        records_update_dbtable(conn, FUNDS_TABLE, ['status', 'checksum', 'urls_to_check', 'access_failures'], funds_to_update)
    if funds_to_check:
        records_to_xlsx(funds_to_check, OUTFILE, OUTPUT_COLS)

    conn.close()

if __name__ == '__main__':
    main()
