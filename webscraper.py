from bs4 import BeautifulSoup
import requests
import hashlib
import time
import sqlite3
from requests.exceptions import HTTPError, Timeout
from utilities import csv_to_dict, dict_to_csv, dbtable_to_dict, dict_update_dbtable
from utilities import db_validate_fieldnames, db_insert, db_update, db_delete

# TODO:
# - cleanup and refactor code
# - implement precheck before executing MOD commands to ensure page changes made b/w command
#       submission and execution are caught
# - improve multi-URL functionality
#   - map URLs to checksums
#   - e.g. two URLs, 1st URL checksum is NULL, 2nd is set. 1st checksum gets set. Program incorrectly detects
#       change in checksum (checksum2 -> checksum1checksum2)
# - revamp input/output system (function and tolerance)
#   - output changed funds to a separate csv file as well, accept input for manual fund status change X
#   - input:
#       - add funds X
#       - delete funds X
#       - modify funds X
#       - get funds table
# - report fund amount if possible
#   - 2 new fields: fund amount and path/id to html element containing fund amount (or maybe find it thru api?)
# - deployment
#   - AWS server w/ simple SQL database?
#   - privileged users need to be able to request/modify data
# - use xlsx instead of csv?
# - checksum specific element(s) instead of checking <body> text
# - 2-bit predictor?
# - proxies
# - NOTE: scraper does not detect page changes if only text generated by js was changed.

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Accept": "text/html,application/xhtml+xml",
    "Accept-Language": "en-US,en;q=0.9"
}

FIELDNAMES = ["name", "url", "status", "urls_to_check"]

OPEN = "Open"
CLOSED = "Closed"
CHECK = "Check Required"

STATUSES = [OPEN, CLOSED, CHECK]

DELIM = ";;"

DATABASE = "webscraper.db"
FUNDS_TABLE = "funds"
INFILE = "input.csv"
OUTFILE = "checkfunds.csv"
AUDITLOG = "auditlog.txt"

def precheck():
    try:
        inputs = csv_to_dict(INFILE)
    except FileNotFoundError as e:
        print(f'FileNotFoundError: {e}. No inputs.')
        return None
    return inputs

def exec_cmd(conn, log, item):
    if set(item.keys()) != {'command', 'name', 'url', 'status'}:
        log.write('INPUT FILE ERROR: Invalid set of column headers\r\n')
        return

    cmd = item.pop('command').upper()

    if not item['name']:
        log.write(f"INPUT ERROR: Empty name for command {cmd}\r\n")
        return
    if not item['url']:
        log.write(f"INPUT ERROR: Empty URL for command {cmd} {item['name']}\r\n")
        return
    if item['status'] not in STATUSES and cmd != 'DEL':
        log.write(f"INPUT ERROR: Invalid status: \"{item['status']}\" for command: {cmd} {item['name']}\r\n")
        return
    try:
        if cmd == 'ADD':
            db_insert(conn, FUNDS_TABLE, item)
            log.write(f"ADD: {item['name']}\r\n")
        elif cmd == 'MOD':
            # TODO: notify user if fund DNE
            item['checksum'] = None
            item['urls_to_check'] = None
            item['access_failures'] = 0
            db_update(conn, FUNDS_TABLE, item)
            log.write(f"MOD: {item['name']}, {item['url']}, {item['status']}\r\n")
        elif cmd == 'DEL':
            db_delete(conn, FUNDS_TABLE, 'name', item['name'])
            log.write(f"DEL: {item['name']}\r\n")
        else:
            log.write(f"INPUT ERROR: Invalid command: {cmd.upper()} {item['name']}\r\n")

    except sqlite3.IntegrityError as e:
        log.write(f"INTEGRITY ERROR: {cmd} {item['name']}: {e}\r\n")
    except Exception as e:
        print(f'Encountered exception: {e}')

def get_soup(url, retries= 3, backoff= 2):
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=(3.1, 15.1))
            if 'text/html' not in response.headers['content-type']:
                raise TypeError
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            return soup
        
        except Timeout as e:
            wait_time = backoff ** attempt
            print(f'TimeoutError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except TypeError as e:
            wait_time = backoff ** attempt
            print(f'TypeError: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)

        except HTTPError as e:
            if e.response.status_code == 412:
                print(f'Error 412. Skipping "{url}"...')
                return None
            wait_time = backoff ** attempt
            print(f'HTTP error: {e}. Retrying in {wait_time} seconds...')
            time.sleep(wait_time)
            
        except Exception as e:
            print(f'Failed to reach URL: {url}. Error: {e}. Skipping...')
            return None
    return None

def check_fund(fund, funds_to_check, funds_to_update):
    urls = fund['url'].split(DELIM)
    urls_to_check = set(fund['urls_to_check'].split(DELIM)) if fund['urls_to_check'] else set()
    old_checksums = fund['checksum'].split(DELIM) if fund['checksum'] else ['' for u in urls]
    checksums = []

    need_update = False
    failed_connect = False
    for i in range(len(urls)):

        soup = get_soup(urls[i])

        if not soup:
            need_update = True
            failed_connect = True
            fund['access_failures'] += 1
    
            if fund['access_failures'] >= 3:
                urls_to_check.add(urls[i])
            
            checksums.append(old_checksums[i])
            print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Failed to connect.")
        else:
            checksums.append(hashlib.sha256("".join(soup.body.text.split()).encode('utf-8')).hexdigest())
        
            if not old_checksums[i]:
                need_update = True
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Adding new checksum.")
            elif checksums[i] != old_checksums[i]:
                need_update = True
                urls_to_check.add(urls[i])
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Updating checksum. Check required.")
            else:
                print(f"{fund['name']}, {fund['status'].upper()} FUND: URL {i+1}: Checksums match.")
    
    if not failed_connect:
        fund['access_failures'] = 0            
    
    if need_update:
        fund['checksum'] = DELIM.join(checksums)
        funds_to_update.append(fund)
    
    if urls_to_check:
        fund['status'] = CHECK
        fund['urls_to_check'] = DELIM.join(urls_to_check)
    
    if fund['status'] == CHECK:
        funds_to_check.append(fund)

    return funds_to_check, funds_to_update

def main():
    inputs = precheck()

    if inputs:
        conn = sqlite3.connect(DATABASE)
        auditlog = open(AUDITLOG, "w")

        for item in inputs:
            exec_cmd(conn, auditlog, item)

        conn.close()
        auditlog.close()

    funds = dbtable_to_dict(DATABASE, FUNDS_TABLE)
    funds_to_check = []
    funds_to_update = []

    for item in funds:
        funds_to_check, funds_to_update = check_fund(item, funds_to_check, funds_to_update)
    
    dict_update_dbtable(funds_to_update, DATABASE, FUNDS_TABLE, ['status', 'checksum', 'urls_to_check', 'access_failures'])
    dict_to_csv(funds_to_check, OUTFILE, FIELDNAMES)

if __name__ == "__main__":
    main()
